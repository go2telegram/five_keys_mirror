# Голосовой интерфейс

Голосовые команды работают поверх OpenAI Audio API (Whisper). Модуль хранит кеш распознанных фрагментов в каталоге `models/audio/`, чтобы повторные обращения к одной и той же записи не требовали повторного обращения к API.

## Включение

1. Укажите ключ OpenAI в `.env`:
   ```env
   OPENAI_API_KEY=sk-...
   ```
2. Включите флаг голосового интерфейса:
   ```env
   ENABLE_VOICE_INTERFACE=true
   ```
3. (Необязательно) Настройте модель и язык:
   ```env
   VOICE_MODEL=gpt-4o-mini-transcribe
   VOICE_LANGUAGE=ru
   ```
4. Убедитесь, что каталог `models/audio/` доступен для записи (создаётся автоматически при запуске).

После перезапуска бота команда `/voice` будет доступна.

## Как пользоваться

- Отправьте команду `/voice` и ответом на неё пришлите голосовое сообщение (voice) или аудиофайл.
- Либо просто отправьте голосовое сообщение: бот распознает его и выполнит соответствующую команду.

Каждое распознавание логируется, а ошибки записываются в логгер `voice.transcribe`.

## Доступные голосовые команды

| Пример фразы | Что делает |
|--------------|------------|
| «Меню», «Старт», «Домой» | Показывает главное меню. |
| «Отчёт», «План», «PDF» | Присылает последний PDF-план. |
| «Ассистент <запрос>» | Передаёт запрос ассистенту OpenAI. |

## Отключение

Для отката достаточно выставить `ENABLE_VOICE_INTERFACE=false` и перезапустить бота. Модуль останется в коде, но обработчики перестанут реагировать на голосовые сообщения.
